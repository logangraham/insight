{"ABD71644-C440-4D2A-A3CF-E840A2D64E05": {"abstract": "Ordnance Survey's research and development team routinely uses machine learning to extract new information from existing data sources. As machine learning is a relatively new field, there is a need to provide high-quality metrics to help understand the data quality of machine learning outputs.\n\nThis project will create a new range of tools and processes to describe and quantify the quality of Ordnance Survey's machine learning outputs. These tools and processes will use different testing methodologies as well as comparative assessments of networks to create benchmarks for accuracy. The project will establish a regulated quality control metric for Ordnance Survey's machine learning models to ensure its processes stand up to the growing accuracy requirements demanded by its widespread customer base.", "title": "OS Machine Learning Metrics", "amount": 26381}, "55EC9B2E-703B-4893-AF35-585D01374D3E": {"abstract": "Songbirds, including familiar species like chaffinches and great tits, share an unusual ability with us: vocal learning. Like us, birds need to hear and imitate others in order to develop their vocal communication signals. Most mammal and vertebrate species cannot do this, including all other primate species apart from us. In recent years, research into the development, neurobiology, and genetics of song learning have revealed ever deeper links between human speech and bird song - so much so that bird song currently represents the best animal model we have for understanding the biology of speech. \n\nIn order to study bird song, researchers need to accurately measure how different songs are from each other. These measures are needed to assess whether one bird really did imitate another, and how precisely they did so. Developing computer algorithms to make such measurements is difficult, however, for many of the same reasons that speech recognition is a difficult task for computers. In this grant, we will use a new approach to solve this problem - inspired by developments in speech recognition. First we will train birds to peck on buttons to get a food reward from a bird feeder, and then train them further to discriminate between different &quot;notes&quot; within bird songs. Then we will train &quot;machine learning&quot; computer algorithms to replicate the birds' decisions. We will thus develop a computer algorithm that we can use to compare bird songs in a way that is biologically validated.\n\nWe will then use our algorithm to investigate how birds learn their songs. To do this, we will make use of data-sets where researchers have simply recorded the different songs sung by birds within the population. This data contains a signature of how the birds actually learned their songs in much the same way that our genomes contain signatures of our evolutionary history. We will exploit this by using a statistical technique in combination with simulation models to infer how birds learn their songs: how frequently they generate new song types due to errors or innovations; who they prefer to learn from; and which songs they prefer to learn. We will do this for 15 different species and populations, allowing us to compare how different groups learn their songs for the first time.", "title": "Machine Learning for Bird Song Learning", "amount": 426688}, "A18BAEFF-95A4-4AE4-821B-905029F9BB19": {"abstract": "Songbirds, including familiar species like chaffinches and great tits, share an unusual ability with us: vocal learning. Like us, birds need to hear and imitate others in order to develop their vocal communication signals. Most mammal and vertebrate species cannot do this, including all other primate species apart from us. In recent years, research into the development, neurobiology, and genetics of song learning have revealed ever deeper links between human speech and bird song - so much so that bird song currently represents the best animal model we have for understanding the biology of speech. \n\nIn order to study bird song, researchers need to accurately measure how different songs are from each other. These measures are needed to assess whether one bird really did imitate another, and how precisely they did so. Developing computer algorithms to make such measurements is difficult, however, for many of the same reasons that speech recognition is a difficult task for computers. In this grant, we will use a new approach to solve this problem - inspired by developments in speech recognition. First we will train birds to peck on buttons to get a food reward from a bird feeder, and then train them further to discriminate between different &quot;notes&quot; within bird songs. Then we will train &quot;machine learning&quot; computer algorithms to replicate the birds' decisions. We will thus develop a computer algorithm that we can use to compare bird songs in a way that is biologically validated.\n\nWe will then use our algorithm to investigate how birds learn their songs. To do this, we will make use of data-sets where researchers have simply recorded the different songs sung by birds within the population. This data contains a signature of how the birds actually learned their songs in much the same way that our genomes contain signatures of our evolutionary history. We will exploit this by using a statistical technique in combination with simulation models to infer how birds learn their songs: how frequently they generate new song types due to errors or innovations; who they prefer to learn from; and which songs they prefer to learn. We will do this for 15 different species and populations, allowing us to compare how different groups learn their songs for the first time.", "title": "Machine Learning for Bird Song Learning", "amount": 535796}, "4DBEFB05-9F9A-433D-905F-02E0AFC0DA70": {"abstract": "Machine learning is a very hot topic in computer science these days. As a world we are generating ever greater volumes of\ndata, and we need to find effective ways to gather and analyse that data, often by searching for regular patterns in data\nsets. The human eye is very good at picking out patterns either from images or from simple time series graphs. However,\nthe human eye comes with its own biases: if you are trying to pick out blips in a single line trace on a screen your selection\nmay not always be the same, but may depend on what has come before. Reproducibility is a huge issue here and one\nwhich impacts any kind of data science: if we are to do an experiment, or pick out interesting features from data, we want to\nmake sure we get the same result every time given the same initial input. Furthermore, as our input data streams get\nbigger and bigger, it is extremely time consuming (and a bit boring!) to look through all the data by eye to pick out the kind\nof features that we want. This is where the extremely powerful tool known as machine learning can help. In this work we\npropose to use machine learning to pick out particular signatures from large catagloues of Space Physics data - but the\ncomputer analysis methods that we will develop will be applicable across multiple disciplines.\n\nThe Space Physics problem we are interested in is called magnetic reconnection: it is a very energetic process which can\ntake place when two oppositely directed magnetic field lines meet, come together, and break. Right before reconnection\nhappens the field lines are holding lots of energy, but as soon as they break this energy can be released into multiple forms\nincluding kinetic energy and thermal energy (heating). The field lines change shape after they break and these newly shaped\nfield lines can &quot;ping&quot; away from the site of reconnection, much like an elastic band that has been snapped. The\nfield lines also carry with them charged particles, and these particles can heat up or change their flow direction as a result\nof the transfer of energy.\n\nIn the solar system everything happens on a giant scale, and magnetic reconnection can involve the magnetic field lines\nand plasma of the Sun and of several magnetised planets, including, but not limited to Mercury, Earth, Jupiter and Saturn.\nSpacecraft flying through the solar system have instruments which can measure magnetic fields and plasmas, and thus\ncan sample any changes associated with reconnection.\n\nThe changes in the shape and orientation of magnetic fields and in the temperature and flow characteristics of charged\nparticles can be observed by spacecraft. When scientists examine spacecraft data to search for evidence of this\nreconnection process, they know what they are looking for in the field and plasma data. There is a huge amount of\nspacecraft data: years and years' worth, with measurements taken several times a second. Reconnection can happen\nevery few minutes at some planets. It would be impossible for a human being to look through all the data and pick out\nevery time reconnection happened in our enormous catalogue.\n\nThe purpose of this research is to teach the computer what reconnection signatures look like to a human eye, and to train\nthe computer to pick these signatures out itself. This technique is called machine learning, and it has many advantages,\nbecause computers can be taught to work more quickly than humans, to give the same answer every time, and to not show\nbiases.\n\nThe ultimate goal at the end of this project is to have trained the computer to select reconnection signatures, and to be able\nto roll out this technique on multiple data sets from the solar system. This will be particularly useful for scientists who want\nto conduct large studies of the behaviour of magnetic fields and plasma across the solar system, under different conditions\nand over multiple years.", "title": "Machine Learning for Space Physics", "amount": 89888}, "4A7EFDAF-4FFE-41C9-84D0-94CD96A76019": {"abstract": "Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.", "title": "WhatML: Watermarking Machine Learning Models", "amount": 21871}, "A477C6FA-CF11-4DDA-8DB7-1FB9432784B1": {"abstract": "Websites are never perfect. They are complex systems and are prone to technical errors. They\nare used by human beings, who do not always behave in expected ways. When humans meet\nsoftware, there will always be a certain amount of trouble. Up to 25% of users struggle when\nthey attempt to make purchases online, and as a result many become frustrated with a website\nand leave without purchasing. Such struggles are believed to cost the UK economy over &pound;1bn\ndue to abandoned online transactions (source: Experian).\nCompanies are increasingly trying to understand the struggles faced by the customer. Digital\nCustomer Experience Management (Digital CEM) solutions have been developed to address\nthis huge business problem. Digital CEM is based on recording user journeys, storing them\nfor analysis, and enabling journeys of interest to be visually replayed alongside technical\ndiagnostics. The resulting insight allows the online retailer to achieve higher sales by a\ndecrease in the number of abandoned transactions.\nUserReplay Limited was founded in 2009, and specialises in &quot;session replay technology&quot;\ndesigned to allow web developers to record, re-run and analyse a visitor's journey through a\nwebsite. This service is intended to measure and optimise the digital experience. We are able\nto find and fix site bugs, resolve disputes, recover abandoned baskets and prevent fraud\nonline.\nThis project seeks to develop a prototype solution which takes the analytical functionality of\nour existing Digital CEM to a completely new level through the use of data mining and\nmachine learning to automate the process of discovery and diagnosis of customer experience\nissues, a process which even with the best tools has previously required a highly trained\nanalyst. In doing so, substantial R&amp;D will be necessary to resolve technical uncertainties.\nThe resultant system will provide a step change in capability which will mark it out from\nexisting alternatives.", "title": "UserReplay Analytics machine learning project", "amount": 249684}, "B475B841-1188-44E8-A3DB-0FBDF52F7748": {"abstract": "Some steps in formal reasoning may be statistical or inductive in nature.\nMany attempts to formalise or exploit this inductive or statistical nature of formal reasoning are related to methods of Neuro-Symbolic Integration, Inductive Logic and Relational Statistical Learning.\nThe proposal is focused on one statistical/inductive aspect of automated theorem proving -- proof-pattern recognition. \n\nHigher-order interactive theorem provers (e.g. HOL or Coq) have been successfully developed into \nsophisticated environments for mechanised proofs. \nWhether these provers are applied to big industrial tasks in software verification, or to formalisation\nof mathematical theories, a programmer may have to tackle thousands of lemmas and theorems of variable sizes and complexities.\nA proof in such languages is constructed by combining a finite number of tactics. Some proofs may yield the same pattern of tactics, \nand can be fully automated, and others may require a user's intervention.\nIn this case, manually found proof for one problematic lemma may serve as a template for several other lemmas needing a manual proof.\nAt present this kind of proof-pattern recognition and recycling is done by hand, and the ML-CAP project will look into methods to automate this. \n\nAnother issue is that unsuccessful attempts of proofs --- in the trial-and-error phase of proof-search, are normally discarded once the proof is found.\nConveniently, analysis of both positive and negative examples is inherent in statistical machine learning. And ML-CAP is going to exploit this.\n\nHowever, applying statistical machine-learning methods to analyse data coming from proof theory is a challenging task for several reasons. \nFormulae written in formal language have a precise, rather than a statistical nature. \nFor example, list(nil) may be a well-formed term, while list(nol) - not; although they may have similar patterns \nrecognisable by machine learning methods.\n\nAnother problem that arises when merging formal logic and statistical machine-learning algorithms is related to their computational complexity.\nMany essential logic algorithms are P-complete and inherently sequential (e.g., first-order unification), while neural networks and other similar devices \nare based on linear algebra and perform parallel computations.\n\nAs a solution to the outlined problems, the coalgebraic approach to automated proofs \nmay provide the right technique of abstraction allowing to analyse proof-patterns using machine learning methods. \nFirstly, coalgebraic computations lend themselves to concurrency, \nand this may be the key to obtaining adequate representation\nof the outlined problems.\nSecondly, they are based on the idea of repeating patterns of potentially infinite computations, rather than outputs of finite computations. \nThese patterns may be detected by methods of statistical pattern recognition. \n\nML-CAP is based upon a novel method of using statistical machine learning in analysis of formal proofs.\nIn summary, it provides algorithms for extracting those features from automated proofs that allow to detect proof patterns \n using statistical machine learning tools, such as neural networks.\nAs a result, neural networks can be trained to distinguish well-formed proofs from ill-formed; distinguish whether a proof belongs to a given family of proofs, \nand even make accurate predictions concerning potential success of a proof-in-progress. All three tasks have serious applications in automated reasoning. \nThe project will aim to generalise this method and develop it into a sound general technique for automated proofs. It will result in new methods useful \nfor a range of researchers in different areas, such as AI, Formal Methods, Coalgebra and Cognitive Science.", "title": "MACHINE LEARNING COALGEBRAIC AUTOMATED PROOFS", "amount": 100268}, "7A7416B8-A0E2-4097-914B-E52130AF821D": {"abstract": "Combinatorial problems are ubiquitous across many sectors in today's world: delivering optimised solutions can lead to considerable economic benefits in many fields such as logistics, packing, design and scheduling (of either people or processes). In a typical scenario, instances (for example, a set of goods to deliver) arrive frequently in a continual stream and a solution needs to be quickly produced. Although there are many well-known approaches to developing optimisation algorithms, most suffer from a problem that is now becoming apparent across the breadth of Artificial Intelligence: systems are limited to performing well on data that is similar to that encountered in their design process, and are unable to adapt when encountering situations outside of their original programming.\n\n\nFor real-world optimisation this is particularly problematic. If optimisers are trained in a one-off process then deployed, the system remains static, despite the fact that optimisation occurs in a dynamic world of changing instance characteristics, changing user-requirements and changes in operating environments that influence solution quality (e.g. breakdowns in a factory or traffic in a city). Such changes may be either gradual, or sudden. In the best case this leads to systems that deliver sub-optimal performance, while at worst, systems that are completely unfit for purpose. Moreover, a system that does not adapt wastes an obvious opportunity to improve its own performance over time as it solves more and more instances.\n\nThe targeted breakthrough of this proposal is to develop a dynamic optimisation system that continually adapts its operating mechanism and its algorithms over time to remain fit-for-purpose - a radical switch from the current one-off design and deployment approach to design of optimisers. The system will:\n\n- Go beyond simply being reactive to being proactive in that it will predict the nature of upcoming instances and speculate about potential future scenarios. In response to these predictions, it will autonomously pre-generate and/or reconfigure suitable algorithms, followed by creation of appropriate mappings from instance to solver, in order to pre-prepare for these future scenarios. It will also respond to user requests to generate instances with specific characteristics and solvers to match them, based on the user's in-depth knowledge of their own business and sector.\n\n- Autonomously improve its own behaviour over time, continually updating its algorithms and methods as it learns from its experience of solving more and more instances.\n\n- Support optimisation with respect to multiple user objectives and requirements via its use of a diverse portfolios of algorithms, that range from those which generate acceptable solutions in a very short time to those that have long running time but deliver the highest possible quality.\n\nTo succeed we will make novel advances in building proactive, continually self-adapting systems and in optimisation/algorithm-selection, enhanced by integration with the latest tools from machine-learning. Benefits will be realised by any business that attempts to optimise their processes in dynamic environments, in which customer demands vary, business requirements change, and the operating environment is subject to unexpected changes. Relevant application domains include (but are not limited to) workforce scheduling, logistics and infrastructure design", "title": "Keep Learning", "amount": 388219}, "F1AE551D-C159-4525-AC18-BB0E65C28435": {"abstract": "Combinatorial problems are ubiquitous across many sectors in today's world: delivering optimised solutions can lead to considerable economic benefits in many fields such as logistics, packing, design and scheduling (of either people or processes). In a typical scenario, instances (for example, a set of goods to deliver) arrive frequently in a continual stream and a solution needs to be quickly produced. Although there are many well-known approaches to developing optimisation algorithms, most suffer from a problem that is now becoming apparent across the breadth of Artificial Intelligence: systems are limited to performing well on data that is similar to that encountered in their design process, and are unable to adapt when encountering situations outside of their original programming.\n\n\nFor real-world optimisation this is particularly problematic. If optimisers are trained in a one-off process then deployed, the system remains static, despite the fact that optimisation occurs in a dynamic world of changing instance characteristics, changing user-requirements and changes in operating environments that influence solution quality (e.g. breakdowns in a factory or traffic in a city). Such changes may be either gradual, or sudden. In the best case this leads to systems that deliver sub-optimal performance, while at worst, systems that are completely unfit for purpose. Moreover, a system that does not adapt wastes an obvious opportunity to improve its own performance over time as it solves more and more instances.\n\nThe targeted breakthrough of this proposal is to develop a dynamic optimisation system that continually adapts its operating mechanism and its algorithms over time to remain fit-for-purpose - a radical switch from the current one-off design and deployment approach to design of optimisers. The system will:\n\n- Go beyond simply being reactive to being proactive in that it will predict the nature of upcoming instances and speculate about potential future scenarios. In response to these predictions, it will autonomously pre-generate and/or reconfigure suitable algorithms, followed by creation of appropriate mappings from instance to solver, in order to pre-prepare for these future scenarios. It will also respond to user requests to generate instances with specific characteristics and solvers to match them, based on the user's in-depth knowledge of their own business and sector.\n\n- Autonomously improve its own behaviour over time, continually updating its algorithms and methods as it learns from its experience of solving more and more instances.\n\n- Support optimisation with respect to multiple user objectives and requirements via its use of a diverse portfolios of algorithms, that range from those which generate acceptable solutions in a very short time to those that have long running time but deliver the highest possible quality.\n\nTo succeed we will make novel advances in building proactive, continually self-adapting systems and in optimisation/algorithm-selection, enhanced by integration with the latest tools from machine-learning. Benefits will be realised by any business that attempts to optimise their processes in dynamic environments, in which customer demands vary, business requirements change, and the operating environment is subject to unexpected changes. Relevant application domains include (but are not limited to) workforce scheduling, logistics and infrastructure design", "title": "Keep Learning", "amount": 378027}, "DD18DE59-317C-47C4-9DBE-19616241FDE2": {"abstract": "Proteins are amongst the most important of all molecules in biological systems. They are crucial to organisms which use them to carry out a huge variety of essential functions: catalysis, transport, storage, motor functions, signalling, chaperoning folding, regulation, molecular recognition, structural roles, and DNA Repair. As proteins are so ubiquitous in biology, understanding their properties is essential if we want to know about biological processes. This project is focused on one of the most significant of all protein functions: enzyme catalysis. Enzymes catalyse, or facilitate, the chemical reactions that occur in living organisms. Understanding how they work is both interesting in itself and useful in areas as diverse as drug design, diagnostics, biofuels, food science and laundry. This project is about the relationship between the structure of a protein and the enzyme function it carries out. We aim to predict the catalytic functionality from a knowledge of the protein structure. In order to achieve this, we will use machine learning methods, and in particular a technique called Random Forest. The forest consists of several hundred 'decision trees', each of which is basically a flow diagram. We will train them to learn patterns in the known properties of existing enzyme structures and the chemistry of the steps comprising the reactions they catalyse. However, the way in which we will generate the trees involves computer-simulated dice-rolling. This will ensure that they are all different, though based on the same underlying information. The decision trees then each make a prediction of the unknown possible catalytic functions. These predictions are treated as votes as to the function of the protein. This voting process produces a consensus of many decision trees and maximises the use of the information contained in the underlying data, generating results which are much more accurate than those of any one decision tree. The prediction of enzyme function is immensely important for a number of reasons. Firstly, being able to predict enzyme function more accurately will improve the functional annotation of genomes and reduce the current risk of misannotations being propagated through bioinformatics databases. Rapid developments in structural genomics, high throughput structure determination of diverse proteins from a wide variety of organisms, mean that many structures are available for enzymes whose functions are not yet known. Secondly, this project will allow us to recognise chemical similarities between evolutionarily unrelated enzymes that catalyse similar steps, though not necessarily similar overall reactions. Thirdly, this work will help us to understand the key determinants of the complex relationship between protein structure, function and evolution, particularly in terms of catalysis of reaction steps. Fourthly, the project will facilitate the design of new enzymes with either novel functions or carefully modified versions of existing functions. This project sits at an interface between disciplines, combining chemistry, biology and computer science. A wide range of skills and expertise is necessary to increase our understanding of catalysis, which has long been an important academic goal. Commercially, this work lays a foundation which is directly useful to the pharmaceutical and biotechnology industries, where enzymes are used both as diagnostics and therapeutics; the agrochemical industry, whose products often target enzymes; in the development of biofuels, which need robust enzymes to improve productivity and reduce costs; in laundry, where enzymes are already used in everyday products; and in the nutrition and food industries. In particular this project will aid in the design of new and repurposed enzymes.", "title": "Machine Learning Approaches to Predict Enzyme Function", "amount": 265110}, "F066F6B0-65C5-4CFA-9E62-493CBA854C6B": {"abstract": "The aim of this network is to establish the UK as the world leading authority in the joint area of Computational Statistics and Machine Learning (CompStat &amp; ML) by advancing communication, interchange and collaboration within the UK between the disciplines of Computational Statistics (CompStat) and Machine Learning (ML).\n\nThe UK has tremendous research strength and depth that is widely acknowledged as world leading in both the individual areas of Computational Statistics and Machine Learning. Despite each of these fields of research developing, largely, independently and having their own separate journals, international societies, conferences and curricula both areas of investigation share a common theoretical foundation based on the underlying formal principles of mathematical statistics and statistical inference. As such there is a natural diffusion of concepts, research and individuals between both disciplines. This network will seek to formalise as well as enhance this interchange and in the process capitalise on important synergies that will emerge from the combined and shared research agendas of CompStat &amp; ML.", "title": "Network on Computational Statistics and Machine Learning", "amount": 104530}, "B15E0AA5-531C-4642-B092-A846028B1CAE": {"abstract": "&quot;Due to their high specificity and low toxicity, antibodies are attractive candidates for improved, more efficient, easier to deliver, and safer therapeutics. In 2016, 5/10 best-selling drugs worldwide were antibodies, including the record-breaking Humira which took &pound;12bn in sales, the highest ever for any pharmaceutical. The global antibody therapeutics market is expected to continue to grow at a remarkable pace, reaching &pound;88bn by 2020, driven by the high incidence of chronic diseases which urgently require more effective treatments. High R&amp;D costs and strict regulatory measures may however remain the longstanding challenges for pharmaceutical companies, hampering market growth.\n\nAntibody discovery is the process of discovering an antibody to bind to a particular antigen. Existing methods are through immunisation of animals (rabbits, mice), or by screening a large antibody library. While these physical platforms are usually able to discover binding antibodies, they require targets that are well-formed and available purified in sufficient quantities. Even then, the process is time consuming and expensive, and existing techniques suffer from throughput, scalability, repeatability and quality issues.\n\nAntiverse is building a world-first computational antibody discovery platform combining state-of-the-art machine learning techniques to predict antibodies that bind to a given antigen target with high affinity. The company has a vision to overturn the &pound;3.5bn antibody drug discovery market, replacing existing antibody discovery techniques with a novel solution to design antibody drugs _in silico_, whilst massively reducing the cost (up to &pound;300K/candidate) and time to discover new candidates, from 3 to 18 months down to just one day.\n\nAntiverse is well placed to exploit this opportunity, having already established a basic model with an initial dataset containing public data to prove technical feasibility. The company has secured laboratory space for generating a proprietary dataset at scale to develop the model and has good industry links and connections with major biopharmaceutical companies and contract research organisations (CROs) that have already expressed a keen interest in trialling the solution once developed.\n\nThis project will enable Antiverse to develop the machine learning algorithm and a large proprietary dataset with at least 10,000 data points required to prove the generative model, facilitating the subsequent development of a platform that can be trialled with industry partners to accelerate the service offering towards commercialisation by mid-2020, helping to establish this UK SME at the forefront of the antibody drug discovery market poised for significant growth.&quot;", "title": "Computational Antibody Design with Machine Learning", "amount": 69748}, "0DFE4BC1-D0DD-422B-A071-99B775B88470": {"abstract": "Antibodies are produced by the body's plasma cells and used by the immune system to combat pathogens that cause disease e.g. bacteria or viruses. An effective antibody should specifically target a component of the pathogen (known as an antigen) and bind to it, resulting in its neutralisation.\n\nAntibody discovery is an expensive and time-consuming process. This limits the pharmaceutical industry's ability to bring new drugs to market swiftly and cost-effectively and therefore delays bringing new treatments to patients who badly need them.\n\nAntiverse is building the world's first computational antibody discovery platform which combines state-of-the-art AI and machine learning techniques to predict which antibodies will bind to a given antigen target with high affinity. This approach could transform the drug discovery landscape and remove the need for lengthy experiments and reduce the number of research animals.\n\nWe have already established feasibility and have developed the machine learning algorithm and a large proprietary dataset to prove the generative model and establish the platform. We are now ready to trial this technology using far greater data sets that will identify potential antibody candidates for further development with pharma partners.", "title": "Antibody Discovery Platform with Machine Learning", "amount": 554598}, "245FDF18-A803-409D-B700-C9AB280157BF": {"abstract": "Machine learning (ML) systems are increasingly being deployed across society, in ways that affect many lives. We must ensure that there are good reasons for us to trust their use. That is, as Baroness Onora O'Neill has said, we should aim for reliable measures of trustworthiness. Three key measures are: \nFairness - measuring and mitigating undesirable bias against individuals or subgroups;\nTransparency/interpretability/explainability - improving our understanding of how ML systems work in real-world applications; and\nRobustness - aiming for reliably good performance even when a system encounters different settings from those in which it was trained.\n\nThis fellowship will advance work on key technical underpinnings of fairness, transparency and robustness of ML systems, and develop timely key applications which work at scale in real world health and criminal justice settings, focusing on interpretability and robustness of medical imaging diagnosis systems, and criminal recidivism prediction. The project will connect with industry, social scientists, ethicists, lawyers, policy makers, stakeholders and the broader public, aiming for two-way engagement - to listen carefully to needs and concerns in order to build the right tools, and in turn to inform policy, users and the public in order to maximise beneficial impacts for society.\n\nThis work is of key national importance for the core UK strategy of being a world leader in safe and ethical AI. As the Prime Minister said in his first speech to the UN, &quot;Can these algorithms be trusted with our lives and our hopes?&quot; If we get this right, we will help ensure fair, transparent benefits across society while protecting citizens from harm, and avoid the potential for a public backlash against AI developments. Without trustworthiness, people will have reason to be afraid of new ML technologies, presenting a barrier to responsible innovation. Trustworthiness removes frictions preventing people from embracing new systems, with great potential to spur economic growth and prosperity in the UK, while delivering equitable benefits for society. Trustworthy ML is a key component of Responsible AI - just announced as one of four key themes of the new Global Partnership on AI.\n\nFurther, this work is needed urgently - ML systems are already being deployed in ways which impact many lives. In particular, healthcare and criminal justice are crucial areas with timely potential to benefit from new technology to improve outcomes, consistency and efficiency, yet there are important ethical concerns which this work will address. The current Covid-19 pandemic, and the Black Lives Matter movement, indicate the urgency of these pressing issues.", "title": "Turing AI Fellowship: Trustworthy Machine Learning", "amount": 1283428}, "FE013D0F-353E-4C3E-89F7-62E30C8032E9": {"abstract": "Every person is biologically unique: even individuals who are superficially similar may show differences at the genetic and biochemical level. This diversity has real implications for medicine, and helps to explain why apparently similar patients often respond very differently to the same therapies. However, we remain limited in our ability to account for such diversity in the clinic. \u201cPrecision medicine\u201d refers to the emerging idea of using molecular measurements (such as gene sequences or protein levels) to match individual patients to the therapies from which they are most likely to benefit. Machine learning (ML) is a field that combines ideas from computer science, mathematics and statistics to find patterns in data. Our research explores how we can exploit the power of statistics and ML to enable precision medicine. Our long-term goal is to develop methods that can exploit large datasets to provide doctors information that can help in treating patients. We work on the mathematical and computational side, inventing new ways to look at complex data that can tease out relevant patterns, and work closely with biomedical researchers in the UK, US and Europe.", "title": "Statistics and machine learning for precision medicine", "amount": 286047}, "89FC94D8-2344-4ED8-B1BE-8E950A546137": {"abstract": "When programmers make mistakes, costs can be huge. Programmers make mistakes because they are human. To help them, researchers are developing tools. No tool is a silver bullet; each has advantages and disadvantages. We focus on one kind of tool: the runtime verifiers. Programmers use runtime verifiers more and more, which attests to their utility. Runtime verifiers observe a program while it runs and look for signs that something is amiss. This slows down the program, which is why runtime verifiers are used only by programmers, before releasing the program to users. If runtime verifiers would be more efficient and would not slow down programs so much, then they could remain active in released programs, which would make them more useful. For example, they could be used for detecting security intrusions in deployed systems.\n\nThis research project is focused on increasing the efficiency of runtime verifiers.\n\n\nA Cambridge study from 2013 estimated that the worldwide cost of fixing software defects to be $310 billion per year. A large part of this cost is programmer time. The interval between finding a defect and fixing it is not so small because the task is often not so easy. Some defects are security vulnerabilities, which makes the fixing interval a window of opportunity for criminals. The window of opportunity is bigger than the fixing interval because it also includes the time until users decide to update their software. The size of the window of opportunity is important. For example, updating software regularly is recommended by the UK government, in its Cyber Essentials Scheme. This guidance was issued after a government study showed that, in 2014, nine out of ten large organisations in UK suffered security breaches, and each security breach had an average cost of 1.5 million pounds.\n\nRuntime verifiers could be used to reduce the window of opportunity that criminals have, as follows. Programmers describe scenarios that should not happen. A runtime verifier then observes the program while it runs and, if one of the forbidden scenarios does happen, then the runtime verifier can take an action. For example, the runtime verifier could be instructed to kill the program, because a program crash is preferable to a security breach. Alas, such a use case for runtime verifiers is unrealistic at the moment, because no user would tolerate a program that runs ten times slower than usual just for the benefit of extra protection against security intrusions. In this project we aim at finding a better trade-off. We want the program to run only slightly slower than usual and, in return, we are willing to give up some of the guarantees offered by a runtime verifier, but not much. But, how to find a good trade-off? The key will be using a toolkit developed recently in the machine learning community.", "title": "Fast Runtime Verification via Machine Learning", "amount": 100917}, "9D584314-0486-40BB-84ED-922985EDE245": {"abstract": "Proteins are recognised as a very important class of molecules because of their versatile functionality in living systems. This proposal addresses the paramount problem of oligopeptide (and ultimately protein) structure prediction from a theoretical and computational point of view. A recent and authoritative review by Ponder and Case (one of the authors of the popular computer program AMBER) argues that biomolecular modelling will grind to a halt unless the accuracy of current force fields is substantially increased. We believe that the best way forward is by starting afresh rather than by tweaking existing force fields. The design of force fields such as AMBER was based on the computing power available in the 1980s. Since that time computing power has increased by a factor 10,000, which means that a novel force field design philosophy can be adopted, avoiding from the outset the approximations of force fields such as AMBER. In our previous work we introduced multipole moments to replace point charges. Multipole moments reflect better the local electron density of an atom than do point charges, which wrongly assume that this density is spherical. Multipole moments model the electrostatic interaction between one atom and another more accurately, especially at short range. We use the modern theory of Quantum Chemical Topology (aka 'Atoms in Molecules') to partition the electron density of small molecules into atomic fragments. These fragments then 'dress up' a protein/peptide backbone and provide detailed information on its electron density. Quantum topological atoms have a finite volume of variable shape, which can be nicely visualised. These atoms are also widely documented, strongly rooted in quantum mechanics and used for interpretative purposes (e.g. charge transfer, hydrogen bonding). While designing a force field along these lines we modeled the pivotal electrostatic interaction energy upfront, without fitting, as is done in constructing classical force fields. Our approach drastically reduces the number of fitted parameters. Moreover fitted charges are not necessarily transferable from small molecules to larger ones. On the other hand, quantum topological atoms are transferable to a very large extent. Only the remaining energy contributions then need to be fitted to 'ab initio' energies, forces and vibrational frequencies of training molecules. In this proposal we focus on the polarisation of the electron density, that is, the change in the electron density upon a change in the nuclear positions. The novel element is to use advanced machine learning to capture the relation between fluctuating multipole moments and nuclear positions. The input of the Genetic Programming algorithm are the coordinates of the neighbouring atoms of a given central atom and the output is a given fluctuating multipole moment of the central atom. If successful, the proposed methodology is expected to work for other important classes of biochemical compounds as well, such as nucleotides (DNA, RNA), carbohydrates and lipids, which will be tackled in future projects. Given an increase in computer power of at least two orders of magnitude occurring over the next decade we aim at guaranteeing a more secure future of macromolecular modeling. Given the momentum built up in our group we are in an ideal position to consolidate all the components of the design, previously researched and published, into a coherent software package that will be freely available to the UK research community.", "title": "Novel force fields devised using machine learning", "amount": 105462}, "52E28BD8-ECD8-4FAC-8FCD-D1960F4E5233": {"abstract": "The aim of this network is to establish the UK as the world leading authority in the joint area of Computational Statistics and Machine Learning (CompStat &amp; ML) by advancing communication, interchange and collaboration within the UK between the disciplines of Computational Statistics (CompStat) and Machine Learning (ML).\n\nThe UK has tremendous research strength and depth that is widely acknowledged as world leading in both the individual areas of Computational Statistics and Machine Learning. Despite each of these fields of research developing, largely, independently and having their own separate journals, international societies, conferences and curricula both areas of investigation share a common theoretical foundation based on the underlying formal principles of mathematical statistics and statistical inference. As such there is a natural diffusion of concepts, research and individuals between both disciplines. This network will seek to formalise as well as enhance this interchange and in the process capitalise on important synergies that will emerge from the combined and shared research agendas of CompStat &amp; ML.", "title": "Network on Computational Statistics and Machine Learning", "amount": 93194}, "6BC7C9E8-A717-438A-9B0E-F35357907105": {"abstract": "The surface topography of a component part can have a profound effect on the function of the part. In tribology, it is the surface interactions that influence such quantities as friction, wear and the lifetime of a component. In fluid dynamics, it is the surface that determines how fluids flow and it affects such properties as aerodynamic lift, therefore, influencing efficiency and fuel consumption of aircraft. Examples of the relationships between the topography of a surface and how that surface functions in use can be found in almost every manufacturing sector, both traditional and high-tech. To control surface topography, and hence the function and/or performance of a component, it must be measured and useful parameters extracted from the measurement data. There are a large number instruments that can measure surface topography, but many of them cannot be used realistically for real-time in-process applications due to the need for scanning in either the lateral axes and/or the vertical axis. There have been developments in area-integrating (scattering) methods for measuring surface topography that can be fast enough to use during a manufacturing process, but these are limited in the height range of surface topography with which they can be used.\n\nIn conventional machining, there has been a significant research effort to determine the surface topography of the machined parts during the manufacturing process. The dominant technology for this has been machine vision approaches, where a relationship between a texture parameter and an aspect of the measured field from an intensity sensor is determined. Such approaches have two major drawbacks: 1. they are usually applied to surfaces with geometrical features over a limited range and 2. they do not have the benefit of a physical model of the measurement process, i.e. they are purely empirical. As an example, the measurement and characterisation of the surface topography of additive manufactured parts remains a significant challenge, especially where measurement speed may be an issue. Typical metal additive manufactured surfaces have a large range of surface features, with the dominant features often being the weld tracks with typical wavelengths of a few hundred micrometres and amplitudes of a few tens of micrometres; such structures are beyond what can be measured effectively with existing commercial approaches. \n\nIn the proposed project, we aim to demonstrate that it is possible to measure rough and structured, machined or additive surfaces using a simple, cost-effective real-time measurement system. This will involve the development of a fully rigorous three-dimensional optical scattering model, which will be combined with a machine learning approach to mine optical scattering data for topographic information that is not within the range of commercial scattering instruments. The proposed system could be mounted into a machining or additive operation without slowing down the process, therefore, reducing the cost of many advanced products that require engineered surfaces. To demonstrate the commercial potential of the project outputs, we have several advanced manufacturing partners who will supply industrially relevant case studies and one partner who could act as the commercial exploitation route for the instrument.", "title": "Revisiting optical scattering with machine learning (SPARKLE)", "amount": 196575}, "9A87E48A-23B8-4A9A-ACD8-300879A10C0A": {"abstract": "A project to evaluate the use of Machine Learning to extend eProcurement in the management of tail end spend. Applegate operates a software-as-a-service eProcurement service called \u201cApplegate PRO\u201d. This allows buyers to submit quote requests for any b2b product or service. This project will investigate the applicability of Machine Learning to identify the most relevant suppliers to match against each quote request.", "title": "Machine Learning for eProcurement Quote Request Matching", "amount": 67863}}